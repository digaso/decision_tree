{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "\n",
    "A decision tree is a supervised machine learning algorithm used for classification and regression tasks, where data is split into branches based on feature values to make predictions. Each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or a continuous value. The tree is built by selecting the best feature to split the data at each step, based on criteria like Gini impurity or entropy for classification, or variance reduction for regression. Despite its simplicity and interpretability, decision trees can be prone to overfitting and sensitive to noisy data, which can be mitigated by techniques like pruning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from sys import argv, exit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "global attribute_possible_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Entropy in the context of decision trees is a measure of impurity or disorder used to determine how a dataset should be split at each node. It quantifies the uncertainty or randomness in the data. In a decision tree, entropy helps to identify the attribute that will best split the data into distinct classes.\n",
    "Definition and Formula\n",
    "\n",
    "Entropy (HH) for a binary classification problem is defined as:\n",
    "H(S)=−p0log⁡2(p0)−p1log⁡2(p1)H(S)=−p0​log2​(p0​)−p1​log2​(p1​)\n",
    "where:\n",
    "\n",
    "    SS is the dataset.\n",
    "    p0p0​ is the proportion of the first class in the dataset.\n",
    "    p1p1​ is the proportion of the second class in the dataset.\n",
    "\n",
    "For a multi-class classification problem, entropy is generalized to:\n",
    "H(S)=−∑i=1npilog⁡2(pi)H(S)=−∑i=1n​pi​log2​(pi​)\n",
    "where:\n",
    "\n",
    "    nn is the number of classes.\n",
    "    pipi​ is the proportion of instances in class ii.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "    High Entropy: Indicates high disorder and mixed classes. If the dataset is evenly split among different classes, entropy is high, meaning the dataset is impure.\n",
    "    Low Entropy: Indicates low disorder and mostly one class. If the dataset contains instances of a single class, entropy is zero, meaning the dataset is pure.\n",
    "\n",
    "### Using Entropy in Decision Trees\n",
    "\n",
    "Calculate Entropy of the Entire Dataset: Measure the overall impurity before any splits.\n",
    "\n",
    "Entropy After a Split: For each candidate attribute, calculate the weighted entropy of the resulting subsets.\n",
    "H(S,A)=∑v∈values(A)∣Sv∣∣S∣H(Sv)H(S,A)=∑v∈values(A)​∣S∣∣Sv​∣​H(Sv​)\n",
    "where:\n",
    "SvSv​ is the subset for which attribute AA has value vv.\n",
    "∣Sv∣∣S∣∣S∣∣Sv​∣​ is the proportion of subset SvSv​ relative to the original dataset SS.\n",
    "Information Gain: \n",
    "Subtract the entropy after the split from the entropy before the split to find the information gain.\n",
    "\n",
    "#### Information Gain\n",
    "    \n",
    "(S,A)=H(S)−H(S,A)Information Gain(S,A)=H(S)−H(S,A)\n",
    "The attribute with the highest information gain is chosen for the split, as it best reduces the uncertainty.\n",
    "\n",
    "By selecting splits that maximize information gain (or equivalently, minimize entropy), the decision tree algorithm ensures that each node split results in the most homogenous child nodes possible, leading to a more accurate and efficient classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(examples, label, possible_labels):\n",
    "    number_rows = examples.shape[0]\n",
    "    entropy_value = 0\n",
    "\n",
    "    for label_value in possible_labels:\n",
    "        number_label_cases = examples[examples[label] == label_value].shape[0]\n",
    "        label_entropy = 0\n",
    "        if number_label_cases > 0:\n",
    "            label_prob = number_label_cases / number_rows\n",
    "            label_entropy = -(label_prob * np.log2(label_prob))\n",
    "        entropy_value += label_entropy\n",
    "    return round(entropy_value, 4)\n",
    "\n",
    "\n",
    "def info_gain(attribute, examples, label, possible_labels):\n",
    "    attr_possible_values = examples[attribute].unique()\n",
    "    number_rows = examples.shape[0]\n",
    "    attr_info_gain = 0.0\n",
    "\n",
    "    for attr_value in attr_possible_values:\n",
    "        attr_value_examples = examples[examples[attribute] == attr_value]\n",
    "        attr_value_number_rows = attr_value_examples.shape[0]\n",
    "        attr_value_entropy = entropy(attr_value_examples, label, possible_labels)\n",
    "        attr_value_prob = attr_value_number_rows / number_rows\n",
    "        attr_info_gain += attr_value_prob * attr_value_entropy\n",
    "\n",
    "    return entropy(examples, label, possible_labels) - attr_info_gain\n",
    "\n",
    "def most_info_gain(examples, label, possible_labels, possible_attributes):\n",
    "    max_info_gain = -1\n",
    "    max_info_attribute = None\n",
    "\n",
    "    for attr in possible_attributes:\n",
    "        attr_info_gain = info_gain(attr, examples, label, possible_labels)\n",
    "        if attr_info_gain > max_info_gain:\n",
    "            max_info_gain = attr_info_gain\n",
    "            max_info_attribute = attr\n",
    "    return max_info_attribute\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identifies the most frequently occurring label within a given set of examples. By extracting the labels from the dataset and utilizing the value_counts() method, it computes the occurrences of each unique label. Subsequently, it employs idxmax() to pinpoint the index associated with the maximum count, effectively revealing the most common label. This function is essential in decision tree algorithms for various tasks such as determining class labels for leaf nodes or guiding pruning processes based on label frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_label(parent_examples):\n",
    "\n",
    "    labels = parent_examples.iloc[:, -1]\n",
    "    return labels.value_counts().idxmax()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function aims to find the optimal split value for a given attribute based on the information gain criterion, which is crucial for building an accurate decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_best_split_value(examples, attribute, label, possible_labels):\n",
    "    \"\"\"\n",
    "    Calcules the best value to split the examples in two subsets, <= and >\n",
    "    :return: The best split value\n",
    "    \"\"\"\n",
    "    attribute_values = sorted(examples[attribute].unique().tolist())\n",
    "    best_split_value = None\n",
    "    best_information_gain = float('-inf')\n",
    "\n",
    "    middle_values = []\n",
    "    for i in range(len(attribute_values) - 1):\n",
    "        middle_values.append((attribute_values[i] + attribute_values[i + 1]) / 2)\n",
    "\n",
    "    if len(attribute_values) == 1:\n",
    "        # All instances have the same value for the attribute\n",
    "        return attribute_values[0]\n",
    "\n",
    "    for value in middle_values:\n",
    "        less_equal = examples[examples[attribute] <= value]\n",
    "        bigger = examples[examples[attribute] > value]\n",
    "\n",
    "        q1 = len(less_equal) / len(examples)\n",
    "        q2 = len(bigger) / len(examples)\n",
    "\n",
    "        entropy1 = entropy(less_equal, label, possible_labels)\n",
    "        entropy2 = entropy(bigger, label, possible_labels)\n",
    "\n",
    "        information_gain = entropy(examples, label, possible_labels) - (q1 * entropy1) - (q2 * entropy2)\n",
    "\n",
    "        if information_gain > best_information_gain:\n",
    "            best_information_gain = information_gain\n",
    "            best_split_value = value\n",
    "\n",
    "    return round(best_split_value, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_branch(attribute, examples, label, possible_labels, parent_examples):\n",
    "    \"\"\"\n",
    "    Generates a branch of the decision tree as a dictionary, as the attribute value\n",
    "    as the key and a tuple of the label value and a counter of the examples that\n",
    "    have that attribute value\n",
    "    :param parent_examples: Parent examples of the examples dataframe\n",
    "    :return: The resulting branch and the next examples that satisfy the branch condition\n",
    "    \"\"\"\n",
    "    attr_values_dict = examples[attribute].value_counts(sort=False)\n",
    "    global attribute_possible_values\n",
    "    possible_val = attribute_possible_values[attribute]\n",
    "    for value in possible_val:\n",
    "        if value not in attr_values_dict.keys():\n",
    "            attr_values_dict[value] = 0\n",
    "    branch = {}\n",
    "    next_examples = examples.copy()  # Cria uma cópia dos exemplos\n",
    "\n",
    "    for attr_value, positives in attr_values_dict.items():\n",
    "        attr_value_examples = examples[examples[attribute] == attr_value]\n",
    "        isPure = False\n",
    "\n",
    "        for label_value in possible_labels:\n",
    "            label_positives = attr_value_examples[attr_value_examples[label] == label_value].shape[0]\n",
    "\n",
    "            if label_positives == positives:\n",
    "                if label_positives == 0 and positives == 0:\n",
    "                    label_value = most_common_label(parent_examples)\n",
    "                branch[attr_value] = (label_value, label_positives)\n",
    "                next_examples = next_examples[next_examples[attribute] != attr_value]\n",
    "                isPure = True\n",
    "\n",
    "        if not isPure:\n",
    "            branch[attr_value] = ('?', -1)\n",
    "\n",
    "    if branch:\n",
    "        return branch, next_examples\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_branch_cont(attribute, examples, label, possible_labels, parent_examples):\n",
    "    \"\"\"\n",
    "    The same as the generate_branch function but with modifications to handle continuous\n",
    "    values. The key of the branch is a condition instead of a specific value\n",
    "    :return: The resulting branch and the next examples that satisfy the branch condition\n",
    "    \"\"\"\n",
    "    best_value_split = calculate_best_split_value(examples, attribute, label, possible_labels)\n",
    "    less_equal = examples[examples[attribute] <= best_value_split]\n",
    "    bigger = examples[examples[attribute] > best_value_split]\n",
    "\n",
    "    next_examples = examples.copy()\n",
    "    branch = {}\n",
    "\n",
    "    isPure = False\n",
    "    for label_value in possible_labels:\n",
    "        label_positives = less_equal[less_equal[label] == label_value].shape[0]\n",
    "        count = less_equal.shape[0]\n",
    "\n",
    "        if label_positives == count:\n",
    "            if label_positives == 0 and count == 0:\n",
    "                label_value = most_common_label(parent_examples)\n",
    "            branch[f\"<= {best_value_split}\"] = (label_value, label_positives)\n",
    "            next_examples = next_examples[next_examples[attribute] > best_value_split]\n",
    "            isPure = True\n",
    "\n",
    "    if not isPure:\n",
    "        branch[f\"<= {best_value_split}\"] = ('?', -1)\n",
    "\n",
    "    isPure = False\n",
    "    for label_value in possible_labels:\n",
    "        label_positives = bigger[bigger[label] == label_value].shape[0]\n",
    "        count = bigger.shape[0]\n",
    "\n",
    "        if label_positives == count:\n",
    "            if label_positives == 0 and count == 0:\n",
    "                label_value = most_common_label(parent_examples)\n",
    "            branch[f\"> {best_value_split}\"] = (label_value, label_positives)\n",
    "            next_examples = next_examples[next_examples[attribute] <= best_value_split]\n",
    "            isPure = True\n",
    "\n",
    "    if not isPure:\n",
    "        branch[f\"> {best_value_split}\"] = ('?', -1)\n",
    "\n",
    "    if branch:\n",
    "        return branch, next_examples, best_value_split\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transverse_tree(tree, row):\n",
    "    \"\"\"\n",
    "    Transverse the decision tree to predict the label/class for the given input row\n",
    "    :param tree: decision tree implemented as a dictionary\n",
    "    :param row: a row of the tests dataset\n",
    "    :return: Predicted value based on the decision tree\n",
    "    \"\"\"\n",
    "    for attr, subtree in tree.items():\n",
    "        value = row[attr]\n",
    "        if isinstance(subtree, dict):\n",
    "\n",
    "            if isinstance(value, str):\n",
    "                if value in subtree:\n",
    "                    subtree = subtree[value]\n",
    "            else:\n",
    "                split_operator, split_value = list(subtree.keys())[0].split()\n",
    "                if split_operator == '<=':\n",
    "                    if value <= float(split_value):\n",
    "                        subtree = subtree['<= ' + split_value]\n",
    "                    else:\n",
    "                        subtree = subtree['> ' + split_value]\n",
    "                else:\n",
    "                    if value > float(split_value):\n",
    "                        subtree = subtree['> ' + split_value]\n",
    "                    else:\n",
    "                        subtree = subtree['<= ' + split_value]\n",
    "\n",
    "            if isinstance(subtree, dict):\n",
    "                return transverse_tree(subtree, row)\n",
    "            else:\n",
    "                return subtree[0]\n",
    "        else:\n",
    "            return subtree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(root, previous_attr_value, examples, label, possible_labels, parent_examples, possible_attributes):\n",
    "    \"\"\"\n",
    "    Recursive function to build the decision tree\n",
    "    :param root: Root of the tree at the moment, represents a node\n",
    "    :param previous_attr_value: Previous value of the previous attribute\n",
    "    :param possible_attributes: List of all possible attributes that is updated each time\n",
    "    an attribute is analysed as the max information gain attribute\n",
    "    :return: Returns when there are no more examples to analyse or no more possible attributes\n",
    "    \"\"\"\n",
    "    if examples.shape[0] != 0:\n",
    "        if not possible_attributes.any():\n",
    "            label_value = most_common_label(parent_examples)\n",
    "            root[previous_attr_value] = (label_value, examples.shape[0])\n",
    "            return\n",
    "        max_info_attr = most_info_gain(examples, label, possible_labels, possible_attributes)\n",
    "        remaining_attr = possible_attributes.drop(max_info_attr)\n",
    "        split_value = None\n",
    "        if examples[max_info_attr].dtype == 'object':\n",
    "            tree, next_examples = generate_branch(max_info_attr, examples, label, possible_labels, parent_examples)\n",
    "            flag = False\n",
    "        else:\n",
    "            tree, next_examples, split_value = generate_branch_cont(max_info_attr, examples, label, possible_labels,\n",
    "                                                                    parent_examples)\n",
    "            flag = True\n",
    "\n",
    "        if previous_attr_value is not None:\n",
    "            root[previous_attr_value] = {}\n",
    "            root[previous_attr_value][max_info_attr] = tree\n",
    "            next_node = root[previous_attr_value][max_info_attr]\n",
    "        else:\n",
    "            root[max_info_attr] = tree\n",
    "            next_node = root[max_info_attr]\n",
    "\n",
    "        for node, branch in list(next_node.items()):\n",
    "            if branch[0] == '?':\n",
    "                if flag:\n",
    "                    if '<=' in node:\n",
    "                        attr_value_examples = next_examples[next_examples[max_info_attr] <= split_value]\n",
    "                    else:\n",
    "                        attr_value_examples = next_examples[next_examples[max_info_attr] > split_value]\n",
    "                else:\n",
    "                    attr_value_examples = next_examples[next_examples[max_info_attr] == node]\n",
    "                build_tree(next_node, node, attr_value_examples, label, possible_labels, examples, remaining_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(tree, indent=''):\n",
    "    \"\"\"\n",
    "    Recursive function to print the decision tree\n",
    "    :param tree: Decision tree as a dicionary\n",
    "    :param indent: Indent of each line that is updated recursively\n",
    "    \"\"\"\n",
    "    if isinstance(tree, dict):\n",
    "        for key, value in tree.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f'{indent}{key}:')\n",
    "                print_tree(value, indent + '  ')\n",
    "            else:\n",
    "                print(f'{indent}{key}: {value[0]}  ({value[1]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(data, label):\n",
    "    \"\"\"\n",
    "    Initialize the ID3 algorithm to build the decision tree\n",
    "    :param data: Training DataFrame\n",
    "    :param label: Label name\n",
    "    :return: The decision tree\n",
    "    \"\"\"\n",
    "    training_data = data.copy()\n",
    "    tree = {}\n",
    "    possible_labels = training_data[label].unique()\n",
    "    id_name = training_data.columns[0]\n",
    "    possible_attributes = training_data.columns.drop([label, id_name])\n",
    "    build_tree(tree, None, training_data, label, possible_labels, training_data, possible_attributes)\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "def main():\n",
    "    parser = ArgumentParser(\"Decision Tree Generator using ID3 Algorithm\")\n",
    "    parser.add_argument('-e', '--examples', help='CSV file name to train the learning tree')\n",
    "    parser.add_argument('-t', '--tests', help='CSV file name to test the learning tree obtained')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if len(argv) == 1:\n",
    "        parser.print_help()\n",
    "        exit(0)\n",
    "\n",
    "    with open(args.examples, 'rt') as db:\n",
    "        training_data = pd.read_csv(db)\n",
    "        global attribute_possible_values\n",
    "        attribute_possible_values = {}\n",
    "        for collumn in training_data.columns:\n",
    "            attribute_possible_values[collumn] = training_data[collumn].unique()\n",
    "        label = training_data.columns[-1]\n",
    "        tree = ID3(training_data, label)\n",
    "        print('Decision Tree:')\n",
    "        print_tree(tree)\n",
    "\n",
    "    if args.tests is not None:\n",
    "        '''Leitura do csv dos testes'''\n",
    "        with open(args.tests, 'rt') as td:\n",
    "            test_data = pd.read_csv(td)\n",
    "\n",
    "            predictions = []\n",
    "            for _, row in test_data.iterrows():\n",
    "                prediction = transverse_tree(tree, row)\n",
    "                predictions.append(prediction)\n",
    "            print(f\"Predictions: {predictions}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
