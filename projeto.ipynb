{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "\n",
    "A decision tree is a supervised machine learning algorithm used for classification and regression tasks, where data is split into branches based on feature values to make predictions. Each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or a continuous value. The tree is built by selecting the best feature to split the data at each step, based on criteria like Gini impurity or entropy for classification, or variance reduction for regression. Despite its simplicity and interpretability, decision trees can be prone to overfitting and sensitive to noisy data, which can be mitigated by techniques like pruning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from sys import argv, exit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "global attribute_possible_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Entropy in the context of decision trees is a measure of impurity or disorder used to determine how a dataset should be split at each node. It quantifies the uncertainty or randomness in the data. In a decision tree, entropy helps to identify the attribute that will best split the data into distinct classes.\n",
    "Definition and Formula\n",
    "\n",
    "Entropy (HH) for a binary classification problem is defined as:\n",
    "H(S)=−p0log⁡2(p0)−p1log⁡2(p1)H(S)=−p0​log2​(p0​)−p1​log2​(p1​)\n",
    "where:\n",
    "\n",
    "    SS is the dataset.\n",
    "    p0p0​ is the proportion of the first class in the dataset.\n",
    "    p1p1​ is the proportion of the second class in the dataset.\n",
    "\n",
    "For a multi-class classification problem, entropy is generalized to:\n",
    "H(S)=−∑i=1npilog⁡2(pi)H(S)=−∑i=1n​pi​log2​(pi​)\n",
    "where:\n",
    "\n",
    "    nn is the number of classes.\n",
    "    pipi​ is the proportion of instances in class ii.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "    High Entropy: Indicates high disorder and mixed classes. If the dataset is evenly split among different classes, entropy is high, meaning the dataset is impure.\n",
    "    Low Entropy: Indicates low disorder and mostly one class. If the dataset contains instances of a single class, entropy is zero, meaning the dataset is pure.\n",
    "\n",
    "### Using Entropy in Decision Trees\n",
    "\n",
    "Calculate Entropy of the Entire Dataset: Measure the overall impurity before any splits.\n",
    "\n",
    "Entropy After a Split: For each candidate attribute, calculate the weighted entropy of the resulting subsets.\n",
    "H(S,A)=∑v∈values(A)∣Sv∣∣S∣H(Sv)H(S,A)=∑v∈values(A)​∣S∣∣Sv​∣​H(Sv​)\n",
    "where:\n",
    "SvSv​ is the subset for which attribute AA has value vv.\n",
    "∣Sv∣∣S∣∣S∣∣Sv​∣​ is the proportion of subset SvSv​ relative to the original dataset SS.\n",
    "Information Gain: \n",
    "Subtract the entropy after the split from the entropy before the split to find the information gain.\n",
    "\n",
    "#### Information Gain\n",
    "    \n",
    "(S,A)=H(S)−H(S,A)Information Gain(S,A)=H(S)−H(S,A)\n",
    "The attribute with the highest information gain is chosen for the split, as it best reduces the uncertainty.\n",
    "\n",
    "By selecting splits that maximize information gain (or equivalently, minimize entropy), the decision tree algorithm ensures that each node split results in the most homogenous child nodes possible, leading to a more accurate and efficient classification model.\n",
    "\n",
    "#### Most info gain \n",
    "Simply returns the biggest information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(examples, label, possible_labels):\n",
    "    number_rows = examples.shape[0]\n",
    "    entropy_value = 0\n",
    "\n",
    "    for label_value in possible_labels:\n",
    "        number_label_cases = examples[examples[label] == label_value].shape[0]\n",
    "        label_entropy = 0\n",
    "        if number_label_cases > 0:\n",
    "            label_prob = number_label_cases / number_rows\n",
    "            label_entropy = -(label_prob * np.log2(label_prob))\n",
    "        entropy_value += label_entropy\n",
    "    return round(entropy_value, 4)\n",
    "\n",
    "\n",
    "def info_gain(attribute, examples, label, possible_labels):\n",
    "    attr_possible_values = examples[attribute].unique()\n",
    "    number_rows = examples.shape[0]\n",
    "    attr_info_gain = 0.0\n",
    "\n",
    "    for attr_value in attr_possible_values:\n",
    "        attr_value_examples = examples[examples[attribute] == attr_value]\n",
    "        attr_value_number_rows = attr_value_examples.shape[0]\n",
    "        attr_value_entropy = entropy(attr_value_examples, label, possible_labels)\n",
    "        attr_value_prob = attr_value_number_rows / number_rows\n",
    "        attr_info_gain += attr_value_prob * attr_value_entropy\n",
    "\n",
    "    return entropy(examples, label, possible_labels) - attr_info_gain\n",
    "\n",
    "def most_info_gain(examples, label, possible_labels, possible_attributes):\n",
    "    max_info_gain = -1\n",
    "    max_info_attribute = None\n",
    "\n",
    "    for attr in possible_attributes:\n",
    "        attr_info_gain = info_gain(attr, examples, label, possible_labels)\n",
    "        if attr_info_gain > max_info_gain:\n",
    "            max_info_gain = attr_info_gain\n",
    "            max_info_attribute = attr\n",
    "    return max_info_attribute\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifies the most frequently occurring label within a given set of examples. By extracting the labels from the dataset and utilizing the value_counts() method, it computes the occurrences of each unique label. Subsequently, it employs idxmax() to pinpoint the index associated with the maximum count, effectively revealing the most common label. This function is essential in decision tree algorithms for various tasks such as determining class labels for leaf nodes or guiding pruning processes based on label frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_label(parent_examples):\n",
    "\n",
    "    labels = parent_examples.iloc[:, -1]\n",
    "    return labels.value_counts().idxmax()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function aims to find the optimal split value for a given attribute based on the information gain criterion, which is crucial for building an accurate decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_best_split_value(examples, attribute, label, possible_labels):\n",
    "    \"\"\"\n",
    "    Calcules the best value to split the examples in two subsets, <= and >\n",
    "    :return: The best split value\n",
    "    \"\"\"\n",
    "    attribute_values = sorted(examples[attribute].unique().tolist())\n",
    "    best_split_value = None\n",
    "    best_information_gain = float('-inf')\n",
    "\n",
    "    middle_values = []\n",
    "    for i in range(len(attribute_values) - 1):\n",
    "        middle_values.append((attribute_values[i] + attribute_values[i + 1]) / 2)\n",
    "\n",
    "    if len(attribute_values) == 1:\n",
    "        # All instances have the same value for the attribute\n",
    "        return attribute_values[0]\n",
    "\n",
    "    for value in middle_values:\n",
    "        less_equal = examples[examples[attribute] <= value]\n",
    "        bigger = examples[examples[attribute] > value]\n",
    "\n",
    "        q1 = len(less_equal) / len(examples)\n",
    "        q2 = len(bigger) / len(examples)\n",
    "\n",
    "        entropy1 = entropy(less_equal, label, possible_labels)\n",
    "        entropy2 = entropy(bigger, label, possible_labels)\n",
    "\n",
    "        information_gain = entropy(examples, label, possible_labels) - (q1 * entropy1) - (q2 * entropy2)\n",
    "\n",
    "        if information_gain > best_information_gain:\n",
    "            best_information_gain = information_gain\n",
    "            best_split_value = value\n",
    "\n",
    "    return round(best_split_value, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_branch(attribute, examples, label, possible_labels, parent_examples):\n",
    "    \"\"\"\n",
    "    Generates a branch of the decision tree as a dictionary, as the attribute value\n",
    "    as the key and a tuple of the label value and a counter of the examples that\n",
    "    have that attribute value\n",
    "    :param parent_examples: Parent examples of the examples dataframe\n",
    "    :return: The resulting branch and the next examples that satisfy the branch condition\n",
    "    \"\"\"\n",
    "    attr_values_dict = examples[attribute].value_counts(sort=False)\n",
    "    global attribute_possible_values\n",
    "    possible_val = attribute_possible_values[attribute]\n",
    "    for value in possible_val:\n",
    "        if value not in attr_values_dict.keys():\n",
    "            attr_values_dict[value] = 0\n",
    "    branch = {}\n",
    "    next_examples = examples.copy()  # Cria uma cópia dos exemplos\n",
    "\n",
    "    for attr_value, positives in attr_values_dict.items():\n",
    "        attr_value_examples = examples[examples[attribute] == attr_value]\n",
    "        isPure = False\n",
    "\n",
    "        for label_value in possible_labels:\n",
    "            label_positives = attr_value_examples[attr_value_examples[label] == label_value].shape[0]\n",
    "\n",
    "            if label_positives == positives:\n",
    "                if label_positives == 0 and positives == 0:\n",
    "                    label_value = most_common_label(parent_examples)\n",
    "                branch[attr_value] = (label_value, label_positives)\n",
    "                next_examples = next_examples[next_examples[attribute] != attr_value]\n",
    "                isPure = True\n",
    "\n",
    "        if not isPure:\n",
    "            branch[attr_value] = ('?', -1)\n",
    "\n",
    "    if branch:\n",
    "        return branch, next_examples\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_branch_cont(attribute, examples, label, possible_labels, parent_examples):\n",
    "    \"\"\"\n",
    "    The same as the generate_branch function but with modifications to handle continuous\n",
    "    values. The key of the branch is a condition instead of a specific value\n",
    "    :return: The resulting branch and the next examples that satisfy the branch condition\n",
    "    \"\"\"\n",
    "    best_value_split = calculate_best_split_value(examples, attribute, label, possible_labels)\n",
    "    less_equal = examples[examples[attribute] <= best_value_split]\n",
    "    bigger = examples[examples[attribute] > best_value_split]\n",
    "\n",
    "    next_examples = examples.copy()\n",
    "    branch = {}\n",
    "\n",
    "    isPure = False\n",
    "    for label_value in possible_labels:\n",
    "        label_positives = less_equal[less_equal[label] == label_value].shape[0]\n",
    "        count = less_equal.shape[0]\n",
    "\n",
    "        if label_positives == count:\n",
    "            if label_positives == 0 and count == 0:\n",
    "                label_value = most_common_label(parent_examples)\n",
    "            branch[f\"<= {best_value_split}\"] = (label_value, label_positives)\n",
    "            next_examples = next_examples[next_examples[attribute] > best_value_split]\n",
    "            isPure = True\n",
    "\n",
    "    if not isPure:\n",
    "        branch[f\"<= {best_value_split}\"] = ('?', -1)\n",
    "\n",
    "    isPure = False\n",
    "    for label_value in possible_labels:\n",
    "        label_positives = bigger[bigger[label] == label_value].shape[0]\n",
    "        count = bigger.shape[0]\n",
    "\n",
    "        if label_positives == count:\n",
    "            if label_positives == 0 and count == 0:\n",
    "                label_value = most_common_label(parent_examples)\n",
    "            branch[f\"> {best_value_split}\"] = (label_value, label_positives)\n",
    "            next_examples = next_examples[next_examples[attribute] <= best_value_split]\n",
    "            isPure = True\n",
    "\n",
    "    if not isPure:\n",
    "        branch[f\"> {best_value_split}\"] = ('?', -1)\n",
    "\n",
    "    if branch:\n",
    "        return branch, next_examples, best_value_split\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transverse Tree function\n",
    "\n",
    "The transverse_tree function is used to make predictions using the decision tree generated by your ID3 algorithm (which will be further explained in the notebook). It is a critical part of our classifier, as it takes a decision tree and a data row from your test dataset and outputs a predicted label for that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transverse_tree(tree, row):\n",
    "    \"\"\"\n",
    "    Transverse the decision tree to predict the label/class for the given input row\n",
    "    :param tree: decision tree implemented as a dictionary\n",
    "    :param row: a row of the tests dataset\n",
    "    :return: Predicted value based on the decision tree\n",
    "    \"\"\"\n",
    "    for attr, subtree in tree.items():\n",
    "        value = row[attr]\n",
    "        if isinstance(subtree, dict):\n",
    "\n",
    "            if isinstance(value, str):\n",
    "                if value in subtree:\n",
    "                    subtree = subtree[value]\n",
    "            else:\n",
    "                split_operator, split_value = list(subtree.keys())[0].split()\n",
    "                if split_operator == '<=':\n",
    "                    if value <= float(split_value):\n",
    "                        subtree = subtree['<= ' + split_value]\n",
    "                    else:\n",
    "                        subtree = subtree['> ' + split_value]\n",
    "                else:\n",
    "                    if value > float(split_value):\n",
    "                        subtree = subtree['> ' + split_value]\n",
    "                    else:\n",
    "                        subtree = subtree['<= ' + split_value]\n",
    "\n",
    "            if isinstance(subtree, dict):\n",
    "                return transverse_tree(subtree, row)\n",
    "            else:\n",
    "                return subtree[0]\n",
    "        else:\n",
    "            return subtree[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Tree Function\n",
    "\n",
    "The build_tree function constructs the decision tree by recursively splitting the dataset based on the attribute that maximizes information gain at each step. It handles both categorical and continuous data and dynamically builds a complex tree structure represented as nested dictionaries. Each node in the dictionary represents a decision point in the tree, directing to either another decision or a final label based on the attributes of the data being classified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(root, previous_attr_value, examples, label, possible_labels, parent_examples, possible_attributes):\n",
    "    \"\"\"\n",
    "    Recursive function to build the decision tree\n",
    "    :param root: Root of the tree at the moment, represents a node\n",
    "    :param previous_attr_value: Previous value of the previous attribute\n",
    "    :param possible_attributes: List of all possible attributes that is updated each time\n",
    "    an attribute is analysed as the max information gain attribute\n",
    "    :return: Returns when there are no more examples to analyse or no more possible attributes\n",
    "    \"\"\"\n",
    "    if examples.shape[0] != 0:\n",
    "        if not possible_attributes.any():\n",
    "            label_value = most_common_label(parent_examples)\n",
    "            root[previous_attr_value] = (label_value, examples.shape[0])\n",
    "            return\n",
    "        max_info_attr = most_info_gain(examples, label, possible_labels, possible_attributes)\n",
    "        remaining_attr = possible_attributes.drop(max_info_attr)\n",
    "        split_value = None\n",
    "        if examples[max_info_attr].dtype == 'object':\n",
    "            tree, next_examples = generate_branch(max_info_attr, examples, label, possible_labels, parent_examples)\n",
    "            flag = False\n",
    "        else:\n",
    "            tree, next_examples, split_value = generate_branch_cont(max_info_attr, examples, label, possible_labels,\n",
    "                                                                    parent_examples)\n",
    "            flag = True\n",
    "\n",
    "        if previous_attr_value is not None:\n",
    "            root[previous_attr_value] = {}\n",
    "            root[previous_attr_value][max_info_attr] = tree\n",
    "            next_node = root[previous_attr_value][max_info_attr]\n",
    "        else:\n",
    "            root[max_info_attr] = tree\n",
    "            next_node = root[max_info_attr]\n",
    "\n",
    "        for node, branch in list(next_node.items()):\n",
    "            if branch[0] == '?':\n",
    "                if flag:\n",
    "                    if '<=' in node:\n",
    "                        attr_value_examples = next_examples[next_examples[max_info_attr] <= split_value]\n",
    "                    else:\n",
    "                        attr_value_examples = next_examples[next_examples[max_info_attr] > split_value]\n",
    "                else:\n",
    "                    attr_value_examples = next_examples[next_examples[max_info_attr] == node]\n",
    "                build_tree(next_node, node, attr_value_examples, label, possible_labels, examples, remaining_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Tree Function\n",
    "\n",
    "The print_tree function is designed to visually represent the decision tree constructed by the ID3 algorithm. It outputs the structure of the tree to the console, using indentation to indicate the depth and hierarchy of the decision nodes and their respective outcomes. This makes it easier to understand how decisions are made within the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(tree, indent=''):\n",
    "    \"\"\"\n",
    "    Recursive function to print the decision tree\n",
    "    :param tree: Decision tree as a dicionary\n",
    "    :param indent: Indent of each line that is updated recursively\n",
    "    \"\"\"\n",
    "    if isinstance(tree, dict):\n",
    "        for key, value in tree.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f'{indent}{key}:')\n",
    "                print_tree(value, indent + '  ')\n",
    "            else:\n",
    "                print(f'{indent}{key}: {value[0]}  ({value[1]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3\n",
    "\n",
    "The ID3 function sets up and begins the process of building a decision tree using the ID3 algorithm. It prepares the necessary parameters and data structures and delegates the complex task of iteratively constructing the tree to the build_tree function. This separation of concerns allows ID3 to focus on initialization and setup, while build_tree handles the algorithmic complexity of creating the tree structure.\n",
    "\n",
    "The output decision tree is a dictionary that can be used for predicting labels of new data instances by traversing it based on the attributes and their values found in the new data. This makes the ID3 function critical for both learning from the training data and preparing the model for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(data, label):\n",
    "    \"\"\"\n",
    "    Initialize the ID3 algorithm to build the decision tree\n",
    "    :param data: Training DataFrame\n",
    "    :param label: Label name\n",
    "    :return: The decision tree\n",
    "    \"\"\"\n",
    "    training_data = data.copy()\n",
    "    tree = {}\n",
    "    possible_labels = training_data[label].unique()\n",
    "    id_name = training_data.columns[0]\n",
    "    possible_attributes = training_data.columns.drop([label, id_name])\n",
    "    build_tree(tree, None, training_data, label, possible_labels, training_data, possible_attributes)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Testing and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_test_split(data, test_size=0.2):\n",
    "    data = data.sample(frac=1).reset_index(drop=True)  # Shuffle the data\n",
    "    split_index = int(len(data) * (1 - test_size))\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    return train_data, test_data\n",
    "\n",
    "def calculate_accuracy(tree, test_data, label):\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for index, row in test_data.iterrows():\n",
    "        prediction = transverse_tree(tree, row)\n",
    "        if prediction == row[label]:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / len(test_data)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('restaurant.csv')\n",
    "\n",
    "global attribute_possible_values\n",
    "attribute_possible_values = {}\n",
    "for collumn in data.columns:\n",
    "    attribute_possible_values[collumn] = data[collumn].unique()\n",
    "\n",
    "train_data, test_data = train_test_split(data)\n",
    "label  = data.columns[-1]\n",
    "\n",
    "tree = ID3(train_data, label)\n",
    "\n",
    "#print_tree(tree)\n",
    "\n",
    "accuracy = calculate_accuracy(tree, test_data, label)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef main():\\n    parser = ArgumentParser(\"Decision Tree Generator using ID3 Algorithm\")\\n    parser.add_argument(\\'-e\\', \\'--examples\\', help=\\'CSV file name to train the learning tree\\')\\n    parser.add_argument(\\'-t\\', \\'--tests\\', help=\\'CSV file name to test the learning tree obtained\\')\\n\\n    args = parser.parse_args()\\n\\n    if len(argv) == 1:\\n        parser.print_help()\\n        exit(0)\\n\\n    with open(args.examples, \\'rt\\') as db:\\n        training_data = pd.read_csv(db)\\n        global attribute_possible_values\\n        attribute_possible_values = {}\\n        for collumn in training_data.columns:\\n            attribute_possible_values[collumn] = training_data[collumn].unique()\\n        label = training_data.columns[-1]\\n        tree = ID3(training_data, label)\\n        print(\\'Decision Tree:\\')\\n        print_tree(tree)\\n\\n    if args.tests is not None:\\n        #Leitura do csv dos testes\\n        with open(args.tests, \\'rt\\') as td:\\n            test_data = pd.read_csv(td)\\n\\n            predictions = []\\n            for _, row in test_data.iterrows():\\n                prediction = transverse_tree(tree, row)\\n                predictions.append(prediction)\\n            print(f\"Predictions: {predictions}\")\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def main():\n",
    "    parser = ArgumentParser(\"Decision Tree Generator using ID3 Algorithm\")\n",
    "    parser.add_argument('-e', '--examples', help='CSV file name to train the learning tree')\n",
    "    parser.add_argument('-t', '--tests', help='CSV file name to test the learning tree obtained')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if len(argv) == 1:\n",
    "        parser.print_help()\n",
    "        exit(0)\n",
    "\n",
    "    with open(args.examples, 'rt') as db:\n",
    "        training_data = pd.read_csv(db)\n",
    "        global attribute_possible_values\n",
    "        attribute_possible_values = {}\n",
    "        for collumn in training_data.columns:\n",
    "            attribute_possible_values[collumn] = training_data[collumn].unique()\n",
    "        label = training_data.columns[-1]\n",
    "        tree = ID3(training_data, label)\n",
    "        print('Decision Tree:')\n",
    "        print_tree(tree)\n",
    "\n",
    "    if args.tests is not None:\n",
    "        #Leitura do csv dos testes\n",
    "        with open(args.tests, 'rt') as td:\n",
    "            test_data = pd.read_csv(td)\n",
    "\n",
    "            predictions = []\n",
    "            for _, row in test_data.iterrows():\n",
    "                prediction = transverse_tree(tree, row)\n",
    "                predictions.append(prediction)\n",
    "            print(f\"Predictions: {predictions}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
