{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "\n",
    "A decision tree is a supervised machine learning algorithm used for classification and regression tasks, where data is split into branches based on feature values to make predictions. Each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or a continuous value. The tree is built by selecting the best feature to split the data at each step, based on criteria like Gini impurity or entropy for classification, or variance reduction for regression. Despite its simplicity and interpretability, decision trees can be prone to overfitting and sensitive to noisy data, which can be mitigated by techniques like pruning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from sys import argv, exit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "global attribute_possible_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Entropy in the context of decision trees is a measure of impurity or disorder used to determine how a dataset should be split at each node. It quantifies the uncertainty or randomness in the data. In a decision tree, entropy helps to identify the attribute that will best split the data into distinct classes.\n",
    "Definition and Formula\n",
    "\n",
    "Entropy (HH) for a binary classification problem is defined as:\n",
    "H(S)=−p0log⁡2(p0)−p1log⁡2(p1)H(S)=−p0​log2​(p0​)−p1​log2​(p1​)\n",
    "where:\n",
    "\n",
    "    SS is the dataset.\n",
    "    p0p0​ is the proportion of the first class in the dataset.\n",
    "    p1p1​ is the proportion of the second class in the dataset.\n",
    "\n",
    "For a multi-class classification problem, entropy is generalized to:\n",
    "H(S)=−∑i=1npilog⁡2(pi)H(S)=−∑i=1n​pi​log2​(pi​)\n",
    "where:\n",
    "\n",
    "    nn is the number of classes.\n",
    "    pipi​ is the proportion of instances in class ii.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "    High Entropy: Indicates high disorder and mixed classes. If the dataset is evenly split among different classes, entropy is high, meaning the dataset is impure.\n",
    "    Low Entropy: Indicates low disorder and mostly one class. If the dataset contains instances of a single class, entropy is zero, meaning the dataset is pure.\n",
    "\n",
    "### Using Entropy in Decision Trees\n",
    "\n",
    "Calculate Entropy of the Entire Dataset: Measure the overall impurity before any splits.\n",
    "\n",
    "Entropy After a Split: For each candidate attribute, calculate the weighted entropy of the resulting subsets.\n",
    "H(S,A)=∑v∈values(A)∣Sv∣∣S∣H(Sv)H(S,A)=∑v∈values(A)​∣S∣∣Sv​∣​H(Sv​)\n",
    "where:\n",
    "SvSv​ is the subset for which attribute AA has value vv.\n",
    "∣Sv∣∣S∣∣S∣∣Sv​∣​ is the proportion of subset SvSv​ relative to the original dataset SS.\n",
    "Information Gain: \n",
    "Subtract the entropy after the split from the entropy before the split to find the information gain.\n",
    "\n",
    "#### Information Gain\n",
    "    \n",
    "(S,A)=H(S)−H(S,A)Information Gain(S,A)=H(S)−H(S,A)\n",
    "The attribute with the highest information gain is chosen for the split, as it best reduces the uncertainty.\n",
    "\n",
    "By selecting splits that maximize information gain (or equivalently, minimize entropy), the decision tree algorithm ensures that each node split results in the most homogenous child nodes possible, leading to a more accurate and efficient classification model.\n",
    "\n",
    "#### Most info gain \n",
    "Simply returns the biggest information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(examples, label, possible_labels):\n",
    "    number_rows = examples.shape[0]\n",
    "    entropy_value = 0\n",
    "\n",
    "    for label_value in possible_labels:\n",
    "        number_label_cases = examples[examples[label] == label_value].shape[0]\n",
    "        label_entropy = 0\n",
    "        if number_label_cases > 0:\n",
    "            label_prob = number_label_cases / number_rows\n",
    "            label_entropy = -(label_prob * np.log2(label_prob))\n",
    "        entropy_value += label_entropy\n",
    "    return round(entropy_value, 4)\n",
    "\n",
    "\n",
    "def info_gain(attribute, examples, label, possible_labels):\n",
    "    attr_possible_values = examples[attribute].unique()\n",
    "    number_rows = examples.shape[0]\n",
    "    attr_info_gain = 0.0\n",
    "\n",
    "    for attr_value in attr_possible_values:\n",
    "        attr_value_examples = examples[examples[attribute] == attr_value]\n",
    "        attr_value_number_rows = attr_value_examples.shape[0]\n",
    "        attr_value_entropy = entropy(attr_value_examples, label, possible_labels)\n",
    "        attr_value_prob = attr_value_number_rows / number_rows\n",
    "        attr_info_gain += attr_value_prob * attr_value_entropy\n",
    "\n",
    "    return entropy(examples, label, possible_labels) - attr_info_gain\n",
    "\n",
    "def most_info_gain(examples, label, possible_labels, possible_attributes):\n",
    "    max_info_gain = -1\n",
    "    max_info_attribute = None\n",
    "\n",
    "    for attr in possible_attributes:\n",
    "        attr_info_gain = info_gain(attr, examples, label, possible_labels)\n",
    "        if attr_info_gain > max_info_gain:\n",
    "            max_info_gain = attr_info_gain\n",
    "            max_info_attribute = attr\n",
    "    return max_info_attribute\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifies the most frequently occurring label within a given set of examples. By extracting the labels from the dataset and utilizing the value_counts() method, it computes the occurrences of each unique label. Subsequently, it employs idxmax() to pinpoint the index associated with the maximum count, effectively revealing the most common label. This function is essential in decision tree algorithms for various tasks such as determining class labels for leaf nodes or guiding pruning processes based on label frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_label(parent_examples):\n",
    "\n",
    "    labels = parent_examples.iloc[:, -1]\n",
    "    return labels.value_counts().idxmax()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function aims to find the optimal split value for a given attribute based on the information gain criterion, which is crucial for building an accurate decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_best_split_value(examples, attribute, label, possible_labels):\n",
    "    \"\"\"\n",
    "    Calcules the best value to split the examples in two subsets, <= and >\n",
    "    :return: The best split value\n",
    "    \"\"\"\n",
    "    attribute_values = sorted(examples[attribute].unique().tolist())\n",
    "    best_split_value = None\n",
    "    best_information_gain = float('-inf')\n",
    "\n",
    "    middle_values = []\n",
    "    for i in range(len(attribute_values) - 1):\n",
    "        middle_values.append((attribute_values[i] + attribute_values[i + 1]) / 2)\n",
    "\n",
    "    if len(attribute_values) == 1:\n",
    "        # All instances have the same value for the attribute\n",
    "        return attribute_values[0]\n",
    "\n",
    "    for value in middle_values:\n",
    "        less_equal = examples[examples[attribute] <= value]\n",
    "        bigger = examples[examples[attribute] > value]\n",
    "\n",
    "        q1 = len(less_equal) / len(examples)\n",
    "        q2 = len(bigger) / len(examples)\n",
    "\n",
    "        entropy1 = entropy(less_equal, label, possible_labels)\n",
    "        entropy2 = entropy(bigger, label, possible_labels)\n",
    "\n",
    "        information_gain = entropy(examples, label, possible_labels) - (q1 * entropy1) - (q2 * entropy2)\n",
    "\n",
    "        if information_gain > best_information_gain:\n",
    "            best_information_gain = information_gain\n",
    "            best_split_value = value\n",
    "\n",
    "    return round(best_split_value, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_branch(attribute, examples, label, possible_labels, parent_examples):\n",
    "    \"\"\"\n",
    "    Generates a branch of the decision tree as a dictionary, as the attribute value\n",
    "    as the key and a tuple of the label value and a counter of the examples that\n",
    "    have that attribute value\n",
    "    :param parent_examples: Parent examples of the examples dataframe\n",
    "    :return: The resulting branch and the next examples that satisfy the branch condition\n",
    "    \"\"\"\n",
    "    attr_values_dict = examples[attribute].value_counts(sort=False)\n",
    "    global attribute_possible_values\n",
    "    possible_val = attribute_possible_values[attribute]\n",
    "    for value in possible_val:\n",
    "        if value not in attr_values_dict.keys():\n",
    "            attr_values_dict[value] = 0\n",
    "    branch = {}\n",
    "    next_examples = examples.copy()  # Cria uma cópia dos exemplos\n",
    "\n",
    "    for attr_value, positives in attr_values_dict.items():\n",
    "        attr_value_examples = examples[examples[attribute] == attr_value]\n",
    "        isPure = False\n",
    "\n",
    "        for label_value in possible_labels:\n",
    "            label_positives = attr_value_examples[attr_value_examples[label] == label_value].shape[0]\n",
    "\n",
    "            if label_positives == positives:\n",
    "                if label_positives == 0 and positives == 0:\n",
    "                    label_value = most_common_label(parent_examples)\n",
    "                branch[attr_value] = (label_value, label_positives)\n",
    "                next_examples = next_examples[next_examples[attribute] != attr_value]\n",
    "                isPure = True\n",
    "\n",
    "        if not isPure:\n",
    "            branch[attr_value] = ('?', -1)\n",
    "\n",
    "    if branch:\n",
    "        return branch, next_examples\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_branch_cont(attribute, examples, label, possible_labels, parent_examples):\n",
    "    \"\"\"\n",
    "    The same as the generate_branch function but with modifications to handle continuous\n",
    "    values. The key of the branch is a condition instead of a specific value\n",
    "    :return: The resulting branch and the next examples that satisfy the branch condition\n",
    "    \"\"\"\n",
    "    best_value_split = calculate_best_split_value(examples, attribute, label, possible_labels)\n",
    "    less_equal = examples[examples[attribute] <= best_value_split]\n",
    "    bigger = examples[examples[attribute] > best_value_split]\n",
    "\n",
    "    next_examples = examples.copy()\n",
    "    branch = {}\n",
    "\n",
    "    isPure = False\n",
    "    for label_value in possible_labels:\n",
    "        label_positives = less_equal[less_equal[label] == label_value].shape[0]\n",
    "        count = less_equal.shape[0]\n",
    "\n",
    "        if label_positives == count:\n",
    "            if label_positives == 0 and count == 0:\n",
    "                label_value = most_common_label(parent_examples)\n",
    "            branch[f\"<= {best_value_split}\"] = (label_value, label_positives)\n",
    "            next_examples = next_examples[next_examples[attribute] > best_value_split]\n",
    "            isPure = True\n",
    "\n",
    "    if not isPure:\n",
    "        branch[f\"<= {best_value_split}\"] = ('?', -1)\n",
    "\n",
    "    isPure = False\n",
    "    for label_value in possible_labels:\n",
    "        label_positives = bigger[bigger[label] == label_value].shape[0]\n",
    "        count = bigger.shape[0]\n",
    "\n",
    "        if label_positives == count:\n",
    "            if label_positives == 0 and count == 0:\n",
    "                label_value = most_common_label(parent_examples)\n",
    "            branch[f\"> {best_value_split}\"] = (label_value, label_positives)\n",
    "            next_examples = next_examples[next_examples[attribute] <= best_value_split]\n",
    "            isPure = True\n",
    "\n",
    "    if not isPure:\n",
    "        branch[f\"> {best_value_split}\"] = ('?', -1)\n",
    "\n",
    "    if branch:\n",
    "        return branch, next_examples, best_value_split\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transverse Tree function\n",
    "\n",
    "The transverse_tree function is used to make predictions using the decision tree generated by your ID3 algorithm (which will be further explained in the notebook). It is a critical part of our classifier, as it takes a decision tree and a data row from your test dataset and outputs a predicted label for that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transverse_tree(tree, row):\n",
    "    \"\"\"\n",
    "    Transverse the decision tree to predict the label/class for the given input row\n",
    "    :param tree: decision tree implemented as a dictionary\n",
    "    :param row: a row of the tests dataset\n",
    "    :return: Predicted value based on the decision tree\n",
    "    \"\"\"\n",
    "    for attr, subtree in tree.items():\n",
    "        value = row[attr]\n",
    "        if isinstance(subtree, dict):\n",
    "\n",
    "            if isinstance(value, str):\n",
    "                if value in subtree:\n",
    "                    subtree = subtree[value]\n",
    "            else:\n",
    "                split_operator, split_value = list(subtree.keys())[0].split()\n",
    "                if split_operator == '<=':\n",
    "                    if value <= float(split_value):\n",
    "                        subtree = subtree['<= ' + split_value]\n",
    "                    else:\n",
    "                        subtree = subtree['> ' + split_value]\n",
    "                else:\n",
    "                    if value > float(split_value):\n",
    "                        subtree = subtree['> ' + split_value]\n",
    "                    else:\n",
    "                        subtree = subtree['<= ' + split_value]\n",
    "\n",
    "            if isinstance(subtree, dict):\n",
    "                return transverse_tree(subtree, row)\n",
    "            else:\n",
    "                return subtree[0]\n",
    "        else:\n",
    "            return subtree[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Tree Function\n",
    "\n",
    "The build_tree function constructs the decision tree by recursively splitting the dataset based on the attribute that maximizes information gain at each step. It handles both categorical and continuous data and dynamically builds a complex tree structure represented as nested dictionaries. Each node in the dictionary represents a decision point in the tree, directing to either another decision or a final label based on the attributes of the data being classified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(root, previous_attr_value, examples, label, possible_labels, parent_examples, possible_attributes):\n",
    "    \"\"\"\n",
    "    Recursive function to build the decision tree\n",
    "    :param root: Root of the tree at the moment, represents a node\n",
    "    :param previous_attr_value: Previous value of the previous attribute\n",
    "    :param possible_attributes: List of all possible attributes that is updated each time\n",
    "    an attribute is analysed as the max information gain attribute\n",
    "    :return: Returns when there are no more examples to analyse or no more possible attributes\n",
    "    \"\"\"\n",
    "    if examples.shape[0] != 0:\n",
    "        if not possible_attributes.any():\n",
    "            label_value = most_common_label(parent_examples)\n",
    "            root[previous_attr_value] = (label_value, examples.shape[0])\n",
    "            return\n",
    "        max_info_attr = most_info_gain(examples, label, possible_labels, possible_attributes)\n",
    "        remaining_attr = possible_attributes.drop(max_info_attr)\n",
    "        split_value = None\n",
    "        if examples[max_info_attr].dtype == 'object':\n",
    "            tree, next_examples = generate_branch(max_info_attr, examples, label, possible_labels, parent_examples)\n",
    "            flag = False\n",
    "        else:\n",
    "            tree, next_examples, split_value = generate_branch_cont(max_info_attr, examples, label, possible_labels,\n",
    "                                                                    parent_examples)\n",
    "            flag = True\n",
    "\n",
    "        if previous_attr_value is not None:\n",
    "            root[previous_attr_value] = {}\n",
    "            root[previous_attr_value][max_info_attr] = tree\n",
    "            next_node = root[previous_attr_value][max_info_attr]\n",
    "        else:\n",
    "            root[max_info_attr] = tree\n",
    "            next_node = root[max_info_attr]\n",
    "\n",
    "        for node, branch in list(next_node.items()):\n",
    "            if branch[0] == '?':\n",
    "                if flag:\n",
    "                    if '<=' in node:\n",
    "                        attr_value_examples = next_examples[next_examples[max_info_attr] <= split_value]\n",
    "                    else:\n",
    "                        attr_value_examples = next_examples[next_examples[max_info_attr] > split_value]\n",
    "                else:\n",
    "                    attr_value_examples = next_examples[next_examples[max_info_attr] == node]\n",
    "                build_tree(next_node, node, attr_value_examples, label, possible_labels, examples, remaining_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Tree Function\n",
    "\n",
    "The print_tree function is designed to visually represent the decision tree constructed by the ID3 algorithm. It outputs the structure of the tree to the console, using indentation to indicate the depth and hierarchy of the decision nodes and their respective outcomes. This makes it easier to understand how decisions are made within the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(tree, indent=''):\n",
    "    \"\"\"\n",
    "    Recursive function to print the decision tree\n",
    "    :param tree: Decision tree as a dicionary\n",
    "    :param indent: Indent of each line that is updated recursively\n",
    "    \"\"\"\n",
    "    if isinstance(tree, dict):\n",
    "        for key, value in tree.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f'{indent}{key}:')\n",
    "                print_tree(value, indent + '  ')\n",
    "            else:\n",
    "                print(f'{indent}{key}: {value[0]}  ({value[1]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3\n",
    "\n",
    "The ID3 function sets up and begins the process of building a decision tree using the ID3 algorithm. It prepares the necessary parameters and data structures and delegates the complex task of iteratively constructing the tree to the build_tree function. This separation of concerns allows ID3 to focus on initialization and setup, while build_tree handles the algorithmic complexity of creating the tree structure.\n",
    "\n",
    "The output decision tree is a dictionary that can be used for predicting labels of new data instances by traversing it based on the attributes and their values found in the new data. This makes the ID3 function critical for both learning from the training data and preparing the model for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(data, label):\n",
    "    \"\"\"\n",
    "    Initialize the ID3 algorithm to build the decision tree\n",
    "    :param data: Training DataFrame\n",
    "    :param label: Label name\n",
    "    :return: The decision tree\n",
    "    \"\"\"\n",
    "    training_data = data.copy()\n",
    "    tree = {}\n",
    "    possible_labels = training_data[label].unique()\n",
    "    id_name = training_data.columns[0]\n",
    "    possible_attributes = training_data.columns.drop([label, id_name])\n",
    "    build_tree(tree, None, training_data, label, possible_labels, training_data, possible_attributes)\n",
    "    return tree\n",
    "\n",
    "# Classifier class\n",
    "class ID3Classifier:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        data = X.copy()\n",
    "        data['label'] = y\n",
    "        self.tree = ID3(data, 'label')\n",
    "        \n",
    "    def predict_instance(self, instance, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        attribute = next(iter(tree))\n",
    "        attribute_value = instance[attribute]\n",
    "        if attribute_value in tree[attribute]:\n",
    "            subtree = tree[attribute][attribute_value]\n",
    "        else:\n",
    "            subtree = next(iter(tree[attribute].values()))  # Fallback to a default subtree if value not seen in training\n",
    "        return self.predict_instance(instance, subtree)    \n",
    "      \n",
    "\n",
    "    # Add this method to handle multi-output predictions\n",
    "    def predict_single_output(self, instance, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        attribute = next(iter(tree))\n",
    "        attribute_value = instance[attribute]\n",
    "        subtree = tree[attribute].get(attribute_value)\n",
    "        if subtree is None:\n",
    "            subtree = next(iter(tree[attribute].values()))  # Fallback to a default subtree if value not seen in training\n",
    "        return self.predict_single_output(instance, subtree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i in range(len(X)):\n",
    "            instance = X.iloc[i]\n",
    "            prediction = self.predict_instance(instance, self.tree)\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and multiclass-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m accuracy_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     38\u001b[0m precision_scores\u001b[38;5;241m.\u001b[39mappend(precision_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     39\u001b[0m recall_scores\u001b[38;5;241m.\u001b[39mappend(recall_score(y_test, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\anaconda3\\envs\\torchEnv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\anaconda3\\envs\\torchEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Rodrigo\\anaconda3\\envs\\torchEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:94\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     96\u001b[0m             type_true, type_pred\n\u001b[0;32m     97\u001b[0m         )\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    101\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and multiclass-multioutput targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have your data loaded and your classifier defined\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('iris.csv')\n",
    "\n",
    "# Assuming the last column is the target and the rest are features\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Define k-fold cross-validation\n",
    "k = 5\n",
    "kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \n",
    "    clf = ID3Classifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Print the average metrics\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores)}\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores)}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores)}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "def main():\n",
    "    parser = ArgumentParser(\"Decision Tree Generator using ID3 Algorithm\")\n",
    "    parser.add_argument('-e', '--examples', help='CSV file name to train the learning tree')\n",
    "    parser.add_argument('-t', '--tests', help='CSV file name to test the learning tree obtained')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if len(argv) == 1:\n",
    "        parser.print_help()\n",
    "        exit(0)\n",
    "\n",
    "    with open(args.examples, 'rt') as db:\n",
    "        training_data = pd.read_csv(db)\n",
    "        global attribute_possible_values\n",
    "        attribute_possible_values = {}\n",
    "        for collumn in training_data.columns:\n",
    "            attribute_possible_values[collumn] = training_data[collumn].unique()\n",
    "        label = training_data.columns[-1]\n",
    "        tree = ID3(training_data, label)\n",
    "        print('Decision Tree:')\n",
    "        print_tree(tree)\n",
    "\n",
    "    if args.tests is not None:\n",
    "        '''Leitura do csv dos testes'''\n",
    "        with open(args.tests, 'rt') as td:\n",
    "            test_data = pd.read_csv(td)\n",
    "\n",
    "            predictions = []\n",
    "            for _, row in test_data.iterrows():\n",
    "                prediction = transverse_tree(tree, row)\n",
    "                predictions.append(prediction)\n",
    "            print(f\"Predictions: {predictions}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
