{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "\n",
    "A decision tree is a supervised machine learning algorithm used for classification and regression tasks, where data is split into branches based on feature values to make predictions. Each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or a continuous value. The tree is built by selecting the best feature to split the data at each step, based on criteria like Gini impurity or entropy for classification, or variance reduction for regression. Despite its simplicity and interpretability, decision trees can be prone to overfitting and sensitive to noisy data, which can be mitigated by techniques like pruning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from sys import argv, exit\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Entropy in the context of decision trees is a measure of impurity or disorder used to determine how a dataset should be split at each node. It quantifies the uncertainty or randomness in the data. In a decision tree, entropy helps to identify the attribute that will best split the data into distinct classes.\n",
    "Definition and Formula\n",
    "\n",
    "Entropy (HH) for a binary classification problem is defined as:\n",
    "H(S)=−p0log⁡2(p0)−p1log⁡2(p1)H(S)=−p0​log2​(p0​)−p1​log2​(p1​)\n",
    "where:\n",
    "\n",
    "    SS is the dataset.\n",
    "    p0p0​ is the proportion of the first class in the dataset.\n",
    "    p1p1​ is the proportion of the second class in the dataset.\n",
    "\n",
    "For a multi-class classification problem, entropy is generalized to:\n",
    "H(S)=−∑i=1npilog⁡2(pi)H(S)=−∑i=1n​pi​log2​(pi​)\n",
    "where:\n",
    "\n",
    "    nn is the number of classes.\n",
    "    pipi​ is the proportion of instances in class ii.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "    High Entropy: Indicates high disorder and mixed classes. If the dataset is evenly split among different classes, entropy is high, meaning the dataset is impure.\n",
    "    Low Entropy: Indicates low disorder and mostly one class. If the dataset contains instances of a single class, entropy is zero, meaning the dataset is pure.\n",
    "\n",
    "### Using Entropy in Decision Trees\n",
    "\n",
    "Calculate Entropy of the Entire Dataset: Measure the overall impurity before any splits.\n",
    "\n",
    "Entropy After a Split: For each candidate attribute, calculate the weighted entropy of the resulting subsets.\n",
    "H(S,A)=∑v∈values(A)∣Sv∣∣S∣H(Sv)H(S,A)=∑v∈values(A)​∣S∣∣Sv​∣​H(Sv​)\n",
    "where:\n",
    "SvSv​ is the subset for which attribute AA has value vv.\n",
    "∣Sv∣∣S∣∣S∣∣Sv​∣​ is the proportion of subset SvSv​ relative to the original dataset SS.\n",
    "Information Gain: \n",
    "Subtract the entropy after the split from the entropy before the split to find the information gain.\n",
    "\n",
    "#### Information Gain\n",
    "    \n",
    "(S,A)=H(S)−H(S,A)Information Gain(S,A)=H(S)−H(S,A)\n",
    "The attribute with the highest information gain is chosen for the split, as it best reduces the uncertainty.\n",
    "\n",
    "By selecting splits that maximize information gain (or equivalently, minimize entropy), the decision tree algorithm ensures that each node split results in the most homogenous child nodes possible, leading to a more accurate and efficient classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(examples, label, possible_labels):\n",
    "    number_rows = examples.shape[0]\n",
    "    entropy_value = 0\n",
    "\n",
    "    for label_value in possible_labels:\n",
    "        number_label_cases = examples[examples[label] == label_value].shape[0]\n",
    "        label_entropy = 0\n",
    "        if number_label_cases > 0:\n",
    "            label_prob = number_label_cases / number_rows\n",
    "            label_entropy = -(label_prob * np.log2(label_prob))\n",
    "        entropy_value += label_entropy\n",
    "    return round(entropy_value, 4)\n",
    "\n",
    "\n",
    "def info_gain(attribute, examples, label, possible_labels):\n",
    "    attr_possible_values = examples[attribute].unique()\n",
    "    number_rows = examples.shape[0]\n",
    "    attr_info_gain = 0.0\n",
    "\n",
    "    for attr_value in attr_possible_values:\n",
    "        attr_value_examples = examples[examples[attribute] == attr_value]\n",
    "        attr_value_number_rows = attr_value_examples.shape[0]\n",
    "        attr_value_entropy = entropy(attr_value_examples, label, possible_labels)\n",
    "        attr_value_prob = attr_value_number_rows / number_rows\n",
    "        attr_info_gain += attr_value_prob * attr_value_entropy\n",
    "\n",
    "    return entropy(examples, label, possible_labels) - attr_info_gain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identifies the most frequently occurring label within a given set of examples. By extracting the labels from the dataset and utilizing the value_counts() method, it computes the occurrences of each unique label. Subsequently, it employs idxmax() to pinpoint the index associated with the maximum count, effectively revealing the most common label. This function is essential in decision tree algorithms for various tasks such as determining class labels for leaf nodes or guiding pruning processes based on label frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_label(parent_examples):\n",
    "\n",
    "    labels = parent_examples.iloc[:, -1]\n",
    "    return labels.value_counts().idxmax()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function aims to find the optimal split value for a given attribute based on the information gain criterion, which is crucial for building an accurate decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_best_split_value(examples, attribute, label, possible_labels):\n",
    "    \"\"\"\n",
    "    Calcules the best value to split the examples in two subsets, <= and >\n",
    "    :return: The best split value\n",
    "    \"\"\"\n",
    "    attribute_values = sorted(examples[attribute].unique().tolist())\n",
    "    best_split_value = None\n",
    "    best_information_gain = float('-inf')\n",
    "\n",
    "    middle_values = []\n",
    "    for i in range(len(attribute_values) - 1):\n",
    "        middle_values.append((attribute_values[i] + attribute_values[i + 1]) / 2)\n",
    "\n",
    "    if len(attribute_values) == 1:\n",
    "        # All instances have the same value for the attribute\n",
    "        return attribute_values[0]\n",
    "\n",
    "    for value in middle_values:\n",
    "        less_equal = examples[examples[attribute] <= value]\n",
    "        bigger = examples[examples[attribute] > value]\n",
    "\n",
    "        q1 = len(less_equal) / len(examples)\n",
    "        q2 = len(bigger) / len(examples)\n",
    "\n",
    "        entropy1 = entropy(less_equal, label, possible_labels)\n",
    "        entropy2 = entropy(bigger, label, possible_labels)\n",
    "\n",
    "        information_gain = entropy(examples, label, possible_labels) - (q1 * entropy1) - (q2 * entropy2)\n",
    "\n",
    "        if information_gain > best_information_gain:\n",
    "            best_information_gain = information_gain\n",
    "            best_split_value = value\n",
    "\n",
    "    return round(best_split_value, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_branch(attribute, examples, label, possible_labels, parent_examples):\n",
    "    \"\"\"\n",
    "    Generates a branch of the decision tree as a dictionary, as the attribute value\n",
    "    as the key and a tuple of the label value and a counter of the examples that\n",
    "    have that attribute value\n",
    "    :param parent_examples: Parent examples of the examples dataframe\n",
    "    :return: The resulting branch and the next examples that satisfy the branch condition\n",
    "    \"\"\"\n",
    "    attr_values_dict = examples[attribute].value_counts(sort=False)\n",
    "    global attribute_possible_values\n",
    "    possible_val = attribute_possible_values[attribute]\n",
    "    for value in possible_val:\n",
    "        if value not in attr_values_dict.keys():\n",
    "            attr_values_dict[value] = 0\n",
    "    branch = {}\n",
    "    next_examples = examples.copy()  # Cria uma cópia dos exemplos\n",
    "\n",
    "    for attr_value, positives in attr_values_dict.items():\n",
    "        attr_value_examples = examples[examples[attribute] == attr_value]\n",
    "        isPure = False\n",
    "\n",
    "        for label_value in possible_labels:\n",
    "            label_positives = attr_value_examples[attr_value_examples[label] == label_value].shape[0]\n",
    "\n",
    "            if label_positives == positives:\n",
    "                if label_positives == 0 and positives == 0:\n",
    "                    label_value = most_common_label(parent_examples)\n",
    "                branch[attr_value] = (label_value, label_positives)\n",
    "                next_examples = next_examples[next_examples[attribute] != attr_value]\n",
    "                isPure = True\n",
    "\n",
    "        if not isPure:\n",
    "            branch[attr_value] = ('?', -1)\n",
    "\n",
    "    if branch:\n",
    "        return branch, next_examples\n",
    "    else:\n",
    "        return None, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
